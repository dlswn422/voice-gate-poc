from __future__ import annotations

"""
ì„ë² ë”© ê¸°ë°˜ 1ì°¨ ì˜ë„ ë¶„ë¥˜ ëª¨ë“ˆ (ìš´ì˜ ìµœì¢…ë³¸ v3 - Intent ì¶•ì†Œ/ì¬ì •ì˜)

ì´ ëª¨ë“ˆì˜ ëª©ì ì€ "ì˜ë„ë¥¼ ì •í™•íˆ ë§íˆëŠ” ê²ƒ"ì´ ì•„ë‹ˆë‹¤.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ì„¤ê³„ ëª©ì 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- ì‚¬ìš©ìì˜ "ë¬¸ì œ ë°œìƒ ë°œí™”"ë¥¼ ë¹ ë¥´ê²Œ ê°ì§€í•œë‹¤
- ë¬¸ì œì˜ 'ëŒ€ëµì ì¸ ì˜ì—­'ë§Œ íƒœê¹…í•œë‹¤ (ê²°ì œ / ë“±ë¡ / ì‹œì„¤ / ë²ˆí˜¸íŒ ì¸ì‹)
- í™•ì‹  ìˆëŠ” ê²½ìš°ì—ë§Œ intentë¥¼ í™•ì •í•œë‹¤
- ì• ë§¤í•œ ê²½ìš°ì—ëŠ” NONEìœ¼ë¡œ ë„˜ê²¨ 2ì°¨(ëŒ€í™”/ì •ì±… ì²˜ë¦¬)ë¡œ ìœ„ì„í•œë‹¤
- LLMì€ ì ˆëŒ€ í˜¸ì¶œí•˜ì§€ ì•ŠëŠ”ë‹¤ (CPU / ì•ˆì •ì„± / ì˜ˆì¸¡ ê°€ëŠ¥ì„±)
"""

import numpy as np
from typing import Dict, List

from sentence_transformers import SentenceTransformer
from src.nlu.intent_schema import Intent, IntentResult


# ==================================================
# Embedding Model
# ==================================================
_EMBEDDING_MODEL = SentenceTransformer(
    "sentence-transformers/all-MiniLM-L6-v2"
)


# ==================================================
# ğŸ”§ ì…ë ¥ ì •ê·œí™” ë ˆì´ì–´
# - ì§§ê³  ì •ë³´ê°€ ë¶€ì¡±í•œ ë°œí™”ë¥¼ "ëª¨ë¸ì´ ì´í•´ ê°€ëŠ¥í•œ ìµœì†Œ ë¬¸ì¥"ìœ¼ë¡œ í™•ì¥
# - ì˜ë¯¸ëŠ” ë°”ê¾¸ì§€ ì•Šê³ , ì‹ í˜¸ë§Œ ì¦í­
# ==================================================
SHORT_ISSUE_EXPANSION: Dict[str, str] = {
    "ì•ˆ ì—´ë ¤ìš”": "ì°¨ë‹¨ê¸°ê°€ ì•ˆ ì—´ë ¤ìš”",
    "ì•ˆì—´ë ¤ìš”": "ì°¨ë‹¨ê¸°ê°€ ì•ˆ ì—´ë ¤ìš”",
    "ì•ˆ ë¼ìš”": "ê¸°ê¸°ê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ì§€ ì•Šì•„ìš”",
    "ì•ˆë¼ìš”": "ê¸°ê¸°ê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ì§€ ì•Šì•„ìš”",
    "ë©ˆì·„ì–´ìš”": "ê¸°ê¸°ê°€ ì‘ë™ ì¤‘ ë©ˆì·„ì–´ìš”",
    "ì´ìƒí•´ìš”": "ê¸°ê¸° ìƒíƒœê°€ ì´ìƒí•´ìš”",
    "ì™œ ì•ˆë¼ìš”": "ê¸°ê¸°ê°€ ì™œ ì‘ë™í•˜ì§€ ì•ŠëŠ”ì§€ ëª¨ë¥´ê² ì–´ìš”",

    # âœ… LPR(ë²ˆí˜¸íŒ) ë‹¨ë¬¸ ë³´ì •
    "ì¸ì‹ì´ ì•ˆë¼ìš”": "ë²ˆí˜¸íŒ ì¸ì‹ì´ ì•ˆ ë¼ìš”",
    "ì¸ì‹ì´ ì•ˆ ë¼ìš”": "ë²ˆí˜¸íŒ ì¸ì‹ì´ ì•ˆ ë¼ìš”",
    "ë²ˆí˜¸íŒì´ ì•ˆë¼ìš”": "ë²ˆí˜¸íŒ ì¸ì‹ì´ ì•ˆ ë¼ìš”",
    "ë²ˆí˜¸íŒ ì•ˆë¼ìš”": "ë²ˆí˜¸íŒ ì¸ì‹ì´ ì•ˆ ë¼ìš”",
    "ë²ˆí˜¸íŒ ì¸ì‹ ì•ˆë¼ìš”": "ë²ˆí˜¸íŒ ì¸ì‹ì´ ì•ˆ ë¼ìš”",
}


def normalize_issue_text(text: str) -> str:
    """
    ì§§ì€ ë¬¸ì œ ë°œí™”ë¥¼ ì˜ë¯¸ ë³´ì¡´ ìƒíƒœë¡œ í™•ì¥í•œë‹¤.
    - LLM âŒ
    - ë£° ê¸°ë°˜ â­•
    - ì‹¤íŒ¨í•´ë„ ì›ë¬¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë¯€ë¡œ ì•ˆì „
    """
    t = (text or "").strip()
    return SHORT_ISSUE_EXPANSION.get(t, t)


# ==================================================
# Intent Prototype ë¬¸ì¥
# - ì‹¤ì œ STT ë°œí™” ìŠ¤íƒ€ì¼ / "í˜„ìƒ ë³´ê³ " ì¤‘ì‹¬
# ==================================================
INTENT_PROTOTYPES: Dict[Intent, List[str]] = {
    Intent.PAYMENT: [
        "ì£¼ì°¨ë¹„ ê²°ì œê°€ ì•ˆ ë¼ìš”",
        "ì •ì‚°ì´ ì•ˆ ë¼ìš”",
        "ì¹´ë“œ ìŠ¹ì¸ ì‹¤íŒ¨ê°€ ë– ìš”",
        "ê²°ì œ ì˜¤ë¥˜ê°€ ë‚˜ìš”",
        "ê²°ì œí–ˆëŠ”ë° ì¶œì°¨ê°€ ì•ˆ ë¼ìš”",
        "ê²°ì œ ì™„ë£Œë¼ê³  ëœ¨ëŠ”ë° ì°¨ë‹¨ê¸°ê°€ ì•ˆ ì—´ë ¤ìš”",
    ],
    Intent.REGISTRATION: [
        "ë°©ë¬¸ì ë“±ë¡ì´ ì•ˆ ë¼ìš”",
        "ë°©ë¬¸ì ë“±ë¡ ì–´ë””ì„œ í•´ìš”",
        "ì •ê¸°ê¶Œ ë“±ë¡ì´ ì•ˆ ë¼ìš”",
        "ë¬´ë£Œì°¨ëŸ‰ ë“±ë¡ì´ ì•ˆ ë¼ìš”",
        "ì°¨ëŸ‰ ë“±ë¡ì´ ì•ˆ ë¼ìš”",
        "ë“±ë¡ì„ í•´ì•¼ í•˜ë‚˜ìš”",
    ],
    Intent.FACILITY: [
        "ì°¨ë‹¨ê¸°ê°€ ê³ ì¥ë‚œ ê²ƒ ê°™ì•„ìš”",
        "í‚¤ì˜¤ìŠ¤í¬ í™”ë©´ì´ ì•ˆ ì¼œì ¸ìš”",
        "ê¸°ê¸°ê°€ ë©ˆì·„ì–´ìš”",
        "í”„ë¦°í„°ê°€ ì•ˆ ë‚˜ì™€ìš”",
        "í†µì‹ ì´ ì•ˆ ë˜ëŠ” ê²ƒ ê°™ì•„ìš”",
        "ê¸°ê¸°ê°€ ì‘ë™ì„ ì•ˆ í•´ìš”",
        "ë²„íŠ¼ì„ ëˆŒëŸ¬ë„ ë°˜ì‘ì´ ì—†ì–´ìš”",
    ],
    Intent.LPR: [
        "ë²ˆí˜¸íŒ ì¸ì‹ì´ ì•ˆ ë¼ìš”",
        "ë²ˆí˜¸íŒì´ ì•ˆ ì°í˜€ìš”",
        "ì°¨ëŸ‰ë²ˆí˜¸ê°€ ì˜ëª» ì¸ì‹ë¼ìš”",
        "ë²ˆí˜¸íŒ ì¸ì‹ ì˜¤ë¥˜ê°€ ë– ìš”",
        "ë²ˆí˜¸íŒì´ ì¸ì‹ë˜ì§€ ì•Šì•˜ë‹¤ê³  ë– ìš”",
        "ì…ì°¨í•  ë•Œ ë²ˆí˜¸íŒ ì¸ì‹ì´ ì•ˆ ë¼ìš”",
        "ì¶œì°¨í•  ë•Œ ë²ˆí˜¸íŒì´ ì•ˆ ì½í˜€ìš”",
        "ë²ˆí˜¸íŒ ì¹´ë©”ë¼ê°€ ì¸ì‹ì„ ëª» í•´ìš”",
        "ë²ˆí˜¸íŒì´ ë‹¬ë¼ìš”",
        "ë²ˆí˜¸íŒì´ ì´ìƒí•´ìš”",
    ],
}


# ==================================================
# Intentë³„ ìë™ í™•ì • threshold
# - ê°’ì´ ë†’ì„ìˆ˜ë¡ ë³´ìˆ˜ì (í™•ì • ì–´ë ¤ì›€), ë‚®ì„ìˆ˜ë¡ ê³µê²©ì (í™•ì • ì‰¬ì›€)
# ==================================================
INTENT_THRESHOLDS: Dict[Intent, float] = {
    Intent.PAYMENT: 0.72,
    Intent.REGISTRATION: 0.70,
    Intent.FACILITY: 0.65,
    Intent.LPR: 0.72,
}


# ==================================================
# Top-1 / Top-2 score ì°¨ì´ ê¸°ì¤€ (gap)
# ==================================================
GAP_THRESHOLD = 0.015


# ==================================================
# í‚¤ì›Œë“œ ê¸°ë°˜ ë¯¸ì„¸ ë³´ì • (ì„ë² ë”© ê²°ê³¼ë¥¼ 'ë’¤ì—ì§€ ì•ŠìŒ')
# ==================================================
KEYWORD_BOOST: Dict[Intent, List[str]] = {
    Intent.PAYMENT: ["ê²°ì œ", "ì¹´ë“œ", "ì •ì‚°", "ìš”ê¸ˆ", "ì£¼ì°¨ë¹„", "ìŠ¹ì¸", "ì‹¤íŒ¨", "ê²°ì œì™„ë£Œ"],
    Intent.REGISTRATION: ["ë“±ë¡", "ì •ê¸°ê¶Œ", "ë¬´ë£Œ", "ë°©ë¬¸", "ë°©ë¬¸ì", "ì°¨ëŸ‰ë“±ë¡", "ë°©ë¬¸ë“±ë¡"],
    Intent.FACILITY: ["ê¸°ê³„", "ê¸°ê¸°", "í‚¤ì˜¤ìŠ¤í¬", "í™”ë©´", "í”„ë¦°í„°", "ì°¨ë‹¨ê¸°", "í†µì‹ ", "ë„¤íŠ¸ì›Œí¬", "í„°ë¯¸ë„", "ì°¨ë‹¨ë´‰"],
    Intent.LPR: ["ë²ˆí˜¸íŒ", "ì¸ì‹", "ì°¨ë²ˆí˜¸", "ì¹´ë©”ë¼", "ì‚¬ì§„", "ì°", "ì½", "ì˜¤ì¸ì‹", "ë‹¤ë¥´", "í‹€ë¦¬", "ë¶ˆì¼ì¹˜", "ë°”ë€Œ", "ë‹¤ë¥´ê²Œ", "ë‹¤ë¥¸", "ë§ì§€"],
}

# ë„ˆë¬´ í‚¤ìš°ë©´ rule-engineì´ ë˜ë¯€ë¡œ ê³¼í•˜ì§€ ì•Šê²Œ
KEYWORD_BOOST_SCORE = 0.06


# ==================================================
# Prototype Embedding ì‚¬ì „ ê³„ì‚°
# ==================================================
_INTENT_EMBEDDINGS: Dict[Intent, np.ndarray] = {}

for intent, sentences in INTENT_PROTOTYPES.items():
    vecs = _EMBEDDING_MODEL.encode(
        sentences,
        normalize_embeddings=True,
    )
    _INTENT_EMBEDDINGS[intent] = np.mean(vecs, axis=0)


def _cosine(a: np.ndarray, b: np.ndarray) -> float:
    """normalize_embeddings=True â†’ dot product = cosine similarity"""
    return float(np.dot(a, b))


# ==================================================
# ğŸš¦ ìµœì¢… 1ì°¨ ì˜ë„ ë¶„ë¥˜
# ==================================================
def detect_intent_embedding(text: str) -> IntentResult:
    """
    ì„ë² ë”© + ë„ë©”ì¸ ë³´ì • ê¸°ë°˜ 1ì°¨ ì˜ë„ ë¶„ë¥˜

    ë°˜í™˜ ê·œì¹™
    ----------
    - í™•ì‹¤í•˜ë©´ â†’ Intent í™•ì •
    - ì• ë§¤í•˜ë©´ â†’ Intent.NONE
    """

    print("\n" + "=" * 60)
    print(f"[INTENT-EMBEDDING] Raw input: {text}")

    if not text or not str(text).strip():
        return IntentResult(intent=Intent.NONE, confidence=0.0)

    # 1) ì…ë ¥ ì •ê·œí™” (ì§§ì€ ë¬¸ì œ ë°œí™” ë³´ì •)
    normalized_text = normalize_issue_text(str(text))
    print(f"[INTENT-EMBEDDING] Normalized input: {normalized_text}")

    # 2) ì‚¬ìš©ì ë°œí™” ì„ë² ë”©
    user_vec = _EMBEDDING_MODEL.encode(
        normalized_text,
        normalize_embeddings=True,
    )

    # 3) intentë³„ ìœ ì‚¬ë„ ê³„ì‚°
    scores: Dict[Intent, float] = {
        intent: _cosine(user_vec, proto_vec)
        for intent, proto_vec in _INTENT_EMBEDDINGS.items()
    }

    # 4) í‚¤ì›Œë“œ ê¸°ë°˜ ë¯¸ì„¸ ë³´ì •
    for intent, keywords in KEYWORD_BOOST.items():
        if any(k in normalized_text for k in keywords):
            scores[intent] += KEYWORD_BOOST_SCORE

    # 5) ì ìˆ˜ ì •ë ¬
    sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)

    for intent, score in sorted_scores:
        print(f"  - {intent.value:<15} : {score:.4f}")

    top_intent, top_score = sorted_scores[0]
    second_score = sorted_scores[1][1] if len(sorted_scores) > 1 else -1.0
    gap = top_score - second_score
    threshold = INTENT_THRESHOLDS.get(top_intent, 0.7)

    confidence = round(float(top_score), 2)

    # 6) ìë™ í™•ì • íŒë‹¨
    if top_score >= threshold and gap >= GAP_THRESHOLD:
        print(f"[INTENT-EMBEDDING] âœ… CONFIRMED â†’ {top_intent.value}")
        return IntentResult(intent=top_intent, confidence=confidence)

    print("[INTENT-EMBEDDING] âš ï¸ AMBIGUOUS â†’ NONE")
    return IntentResult(intent=Intent.NONE, confidence=confidence)
