# src/nlu/llm_client.py
from __future__ import annotations

import json
import os
import re
import time
import traceback
import requests

from src.nlu.intent_schema import IntentResult, Intent


# ==================================================
# Ollama Native Chat API ì„¤ì • (Intent-1 ì „ìš©)
# ==================================================
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434").rstrip("/")
OLLAMA_MODEL = os.getenv(
    "OLLAMA_INTENT_MODEL",
    os.getenv("OLLAMA_MODEL", "llama3.1:8b"),
)
OLLAMA_TIMEOUT = float(os.getenv("OLLAMA_TIMEOUT", "120"))

OLLAMA_CHAT_URL = f"{OLLAMA_BASE_URL}/api/chat"


# ==================================================
# 1ì°¨ ì˜ë„ ë¶„ë¥˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (LEVEL-1 ONLY)
# ==================================================
SYSTEM_PROMPT_INTENT = (
    "ë„ˆëŠ” ì£¼ì°¨ì¥ í‚¤ì˜¤ìŠ¤í¬ ìŒì„± ì‹œìŠ¤í…œì˜ 1ì°¨ ì˜ë„ ë¶„ë¥˜ê¸°ë‹¤.\n\n"
    "ì—­í• :\n"
    "- ì‚¬ìš©ìì˜ ë°œí™”ë¥¼ 'ì£¼ì œ ë‹¨ìœ„(Level-1 Intent)'ë¡œë§Œ ë¶„ë¥˜í•œë‹¤\n"
    "- í•´ê²° ë°©ë²• ì œì‹œ, ì‹¤í–‰ íŒë‹¨, ëŒ€í™” ìƒì„±ì€ ì ˆëŒ€ í•˜ì§€ ì•ŠëŠ”ë‹¤\n"
    "- HOW_TO, ISSUE, ERROR ê°™ì€ ì„¸ë¶€ ì›ì¸ì€ ê³ ë ¤í•˜ì§€ ì•ŠëŠ”ë‹¤\n\n"
    "[ì˜ë„ ëª©ë¡]\n"
    "- ENTRY        (ì…ì°¨ ê´€ë ¨)\n"
    "- EXIT         (ì¶œì°¨ ê´€ë ¨)\n"
    "- PAYMENT      (ìš”ê¸ˆ/ê²°ì œ/ì •ì‚° ê´€ë ¨)\n"
    "- REGISTRATION (ë°©ë¬¸ì/ì°¨ëŸ‰ ë“±ë¡ ê´€ë ¨)\n"
    "- TIME_PRICE   (ì‹œê°„/ìš”ê¸ˆ ì •ì±… ë¬¸ì˜)\n"
    "- FACILITY     (ì°¨ë‹¨ê¸°/ê¸°ê¸° ì´ìƒ)\n"
    "- COMPLAINT    (ë¶ˆë§Œ/ì§œì¦/í˜¼ë€ í‘œí˜„)\n"
    "- NONE         (ì£¼ì°¨ì¥ê³¼ ë¬´ê´€)\n\n"
    "[ë¶„ë¥˜ ê·œì¹™]\n"
    "- ëª…ë ¹ì²˜ëŸ¼ ë³´ì—¬ë„ 'í–‰ë™'ì´ ì•„ë‹Œ 'ì£¼ì œ'ë¡œ ë¶„ë¥˜í•œë‹¤\n"
    "- ë¬¸ì œ ìƒí™©ê³¼ ë°©ë²• ë¬¸ì˜ë¥¼ êµ¬ë¶„í•˜ì§€ ì•ŠëŠ”ë‹¤\n"
    "- ì• ë§¤í•´ë„ ë°˜ë“œì‹œ í•˜ë‚˜ì˜ ì˜ë„ë¥¼ ì„ íƒí•œë‹¤\n\n"
    "[ì¶œë ¥ ê·œì¹™]\n"
    "- ë°˜ë“œì‹œ JSONë§Œ ì¶œë ¥í•œë‹¤\n"
    "- í˜•ì‹: {\"intent\": \"INTENT_NAME\"}\n"
    "- ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ì•ŠëŠ”ë‹¤\n"
)


# ==================================================
# JSON ì¶”ì¶œ ìœ í‹¸ (ë°©ì–´ì )
# ==================================================
def _extract_json(text: str) -> dict:
    """
    LLM ì¶œë ¥ì—ì„œ intent JSONì„ ìµœëŒ€í•œ ì•ˆì „í•˜ê²Œ ì¶”ì¶œí•œë‹¤.

    í—ˆìš© ì¼€ì´ìŠ¤:
    - ìˆœìˆ˜ JSON
    - ì½”ë“œë¸”ë¡ í¬í•¨ JSON (```json ... ```)
    - ì„¤ëª… + JSON
    - JSONì´ ì¡°ê¸ˆ ê¹¨ì¡Œì§€ë§Œ intent í‚¤ëŠ” ì¡´ì¬
    """
    if not text:
        raise ValueError("Empty LLM output")

    t = text.strip()

    # 1) ê°€ì¥ í° JSON ë¸”ë¡(ì²« '{' ~ ë§ˆì§€ë§‰ '}') ì‹œë„
    start = t.find("{")
    end = t.rfind("}") + 1
    if start != -1 and end > start:
        cand = t[start:end].strip()
        try:
            return json.loads(cand)
        except Exception:
            pass

    # 2) ê°€ì¥ ì²« JSON ê°ì²´(ì§§ì€ {...})ë¼ë„ ì°¾ì•„ë³´ê¸°
    m = re.search(r"\{.*?\}", t, flags=re.S)
    if m:
        try:
            return json.loads(m.group(0))
        except Exception:
            pass

    # 3) fallback: intent í‚¤ë§Œ ê°•ì œ ì¶”ì¶œ
    m = re.search(r'"intent"\s*:\s*"([A-Z_]+)"', t)
    if m:
        return {"intent": m.group(1)}

    raise ValueError(f"JSON not found in output: {t}")


# ==================================================
# 1ì°¨ ì˜ë„ ë¶„ë¥˜ (INTENT ONLY)
# ==================================================
def detect_intent_llm(text: str, debug: bool = True) -> IntentResult:
    """
    1ì°¨(Level-1) ì˜ë„ ë¶„ë¥˜ ì „ìš© í•¨ìˆ˜

    ì…ë ¥:
        - STTë¡œ í™•ì •ëœ ì‚¬ìš©ì ë°œí™”

    ì¶œë ¥:
        - IntentResult(intent, confidence=0.0)

    âš ï¸ ì£¼ì˜
    - ì´ í•¨ìˆ˜ëŠ” ì ˆëŒ€ í•´ê²°í•˜ì§€ ì•ŠëŠ”ë‹¤
    - confidenceëŠ” AppEngineì—ì„œ ê³„ì‚°í•œë‹¤
    """
    if not text or not text.strip():
        return IntentResult(intent=Intent.NONE, confidence=0.0)

    if debug:
        print(f"[LLM] (Intent-1) Input text: {text}")
        print(f"[LLM] (Intent-1) model={OLLAMA_MODEL}")

    prompt = SYSTEM_PROMPT_INTENT + "\n\n[ì‚¬ìš©ì ë°œí™”]\n" + text

    payload = {
        "model": OLLAMA_MODEL,
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "stream": False,
        "options": {
            # ë¶„ë¥˜ëŠ” í”ë“¤ë¦¬ë©´ ì•ˆ ë¨
            "temperature": 0.0,
            # JSON í•˜ë‚˜ë§Œ ì¶œë ¥í•˜ë©´ ì¶©ë¶„
            "num_predict": 32,
        },
    }

    try:
        if debug:
            print("[LLM] â³ Intent-1 inference started...")
        start_ts = time.time()

        r = requests.post(
            OLLAMA_CHAT_URL,
            json=payload,
            timeout=OLLAMA_TIMEOUT,
        )
        r.raise_for_status()

        elapsed_ms = (time.time() - start_ts) * 1000
        if debug:
            print(f"[LLM] âœ… Intent-1 inference finished ({elapsed_ms:.0f} ms)")

        data = r.json()
        content = (data.get("message") or {}).get("content", "") or ""

        if debug:
            print("[LLM] (Intent-1) Raw output:")
            print(content)

        obj = _extract_json(content)
        intent_str = str(obj.get("intent", "NONE")).strip()

        try:
            intent = Intent(intent_str)
        except Exception:
            intent = Intent.NONE

        if debug:
            print(f"[LLM] ğŸ¯ Intent-1 classified: {intent.name}")

        return IntentResult(intent=intent, confidence=0.0)

    except Exception as e:
        print("[LLM] âŒ Intent-1 inference failed")
        if debug:
            print(repr(e))
            traceback.print_exc()

        # ì‹¤íŒ¨ ì‹œì—ë„ ì‹œìŠ¤í…œì€ ë©ˆì¶”ì§€ ì•ŠëŠ”ë‹¤
        return IntentResult(intent=Intent.NONE, confidence=0.0)
