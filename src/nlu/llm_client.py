# src/nlu/llm_client.py
from __future__ import annotations

import json
import os
import re
import time
import traceback
import requests

from src.nlu.intent_schema import IntentResult, Intent

# ==================================================
# Ollama Native Chat API ÏÑ§Ï†ï (ÌôïÏ†ï)
# ==================================================
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434").rstrip("/")
OLLAMA_MODEL = os.getenv(
    "OLLAMA_INTENT_MODEL",
    os.getenv("OLLAMA_MODEL", "llama3.1:8b"),
)
OLLAMA_TIMEOUT = float(os.getenv("OLLAMA_TIMEOUT", "20"))

OLLAMA_CHAT_URL = f"{OLLAMA_BASE_URL}/api/chat"

_JSON_RE = re.compile(r"\{[\s\S]*?\}")

# ==================================================
# 1Ï∞® ÏùòÎèÑ Î∂ÑÎ•ò ÏãúÏä§ÌÖú ÌîÑÎ°¨ÌîÑÌä∏ (INTENT ONLY)
# ==================================================
SYSTEM_PROMPT_INTENT = (
    "ÎÑàÎäî 'Ï£ºÏ∞®Ïû• ÌÇ§Ïò§Ïä§ÌÅ¨ CX' Ï†ÑÏö© ÏùåÏÑ± ÏùòÎèÑ Î∂ÑÎ•ò AIÎã§.\n\n"
    "ÏÇ¨Ïö©ÏûêÏùò Î∞úÌôîÎ•º ÏïÑÎûò ÏùòÎèÑ Ï§ë ÌïòÎÇòÎ°úÎßå Î∂ÑÎ•òÌïòÎùº.\n"
    "Ïù¥ Î∂ÑÎ•òÎäî Ïã§ÌñâÏù¥ÎÇò Ï†úÏñ¥ ÌåêÎã®Ïù¥ ÏïÑÎãàÎùº,\n"
    "ÏÇ¨Ïö©ÏûêÍ∞Ä Ï≤òÌïú ÏÉÅÌô© / Î¨∏Ï†ú / Î¨∏Ïùò Ïú†ÌòïÏùÑ Íµ¨Î∂ÑÌïòÍ∏∞ ÏúÑÌïú Í≤ÉÏù¥Îã§.\n\n"
    "[ÏùòÎèÑ Î™©Î°ù]\n"
    "- ENTRY_FLOW_ISSUE\n"
    "- EXIT_FLOW_ISSUE\n"
    "- PAYMENT_ISSUE\n"
    "- REGISTRATION_ISSUE\n"
    "- TIME_ISSUE\n"
    "- PRICE_INQUIRY\n"
    "- HOW_TO_EXIT\n"
    "- HOW_TO_REGISTER\n"
    "- COMPLAINT\n"
    "- NONE\n\n"
    "[Î∂ÑÎ•ò Í∑úÏπô]\n"
    "- Î™ÖÎ†πÏ≤òÎüº Î≥¥Ïó¨ÎèÑ ÏÉÅÌô© ÏÑ§Î™ÖÏúºÎ°ú Î≥∏Îã§\n"
    "- ÏûÖÏ∞® Î¨∏Ï†úÏôÄ Ï∂úÏ∞® Î¨∏Ï†úÎ•º Î¨∏Îß•ÏúºÎ°ú Íµ¨Î∂ÑÌïúÎã§\n"
    "- Ïï†Îß§Ìï¥ÎèÑ Î∞òÎìúÏãú ÌïòÎÇòÏùò ÏùòÎèÑÎßå ÏÑ†ÌÉùÌïúÎã§\n"
    "- Ìï¥Í≤∞ Î∞©Î≤ïÏù¥ÎÇò Ïã§Ìñâ ÌåêÎã®ÏùÄ Ï†àÎåÄ ÌïòÏßÄ ÏïäÎäîÎã§\n\n"
    "[Ï∂úÎ†• Í∑úÏπô]\n"
    "- Î∞òÎìúÏãú JSONÎßå Ï∂úÎ†•ÌïúÎã§\n"
    "- ÌòïÏãù: {\"intent\": \"INTENT_NAME\"}\n"
    "- Îã§Î•∏ ÌÖçÏä§Ìä∏ Ï∂úÎ†• Í∏àÏßÄ\n"
)

# ==================================================
# JSON Ï∂îÏ∂ú Ïú†Ìã∏
# ==================================================
def _extract_json(text: str) -> dict:
    text = (text or "").strip()

    start = text.find("{")
    end = text.rfind("}") + 1
    if start != -1 and end > start:
        return json.loads(text[start:end])

    m = _JSON_RE.search(text)
    if m:
        return json.loads(m.group(0))

    raise ValueError(f"JSON not found in output: {text}")

# ==================================================
# 1Ï∞® ÏùòÎèÑ Î∂ÑÎ•ò (INTENT ONLY)
# ==================================================
def detect_intent_llm(text: str, debug: bool = True) -> IntentResult:
    """
    - 1Ï∞® intent Î∂ÑÎ•ò Ï†ÑÏö©
    - confidenceÎäî AppEngineÏóêÏÑú Í≥ÑÏÇ∞
    """

    if not text or not text.strip():
        return IntentResult(intent=Intent.NONE, confidence=0.0)

    if debug:
        print(f"[LLM] (Ollama) Input text: {text}")
        print(f"[LLM] (Ollama) base_url={OLLAMA_BASE_URL}")
        print(f"[LLM] (Ollama) model={OLLAMA_MODEL}")

    prompt = (
        SYSTEM_PROMPT_INTENT
        + "\n\n[ÏÇ¨Ïö©Ïûê Î∞úÌôî]\n"
        + text
    )

    payload = {
        "model": OLLAMA_MODEL,
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "stream": False,
        "options": {
            "temperature": 0.0,
            "num_predict": 32,
        },
    }

    try:
        print("[LLM] ‚è≥ Intent inference started...")
        start_ts = time.time()

        r = requests.post(
            OLLAMA_CHAT_URL,
            json=payload,
            timeout=OLLAMA_TIMEOUT,
        )
        r.raise_for_status()

        elapsed_ms = (time.time() - start_ts) * 1000
        print(f"[LLM] ‚úÖ Intent inference finished ({elapsed_ms:.0f} ms)")

        data = r.json()
        content = data.get("message", {}).get("content", "")

        if debug:
            print("[LLM] (Ollama) Raw output:")
            print(content)

        obj = _extract_json(content)
        intent_str = str(obj.get("intent", "NONE")).strip()

        try:
            intent = Intent(intent_str)
        except Exception:
            intent = Intent.NONE

        print(f"[LLM] üéØ Parsed intent: {intent.name}")

        return IntentResult(
            intent=intent,
            confidence=0.0,
        )

    except Exception as e:
        print("[LLM] ‚ùå Intent inference failed")
        if debug:
            print(repr(e))
            traceback.print_exc()

        return IntentResult(
            intent=Intent.NONE,
            confidence=0.0,
        )
