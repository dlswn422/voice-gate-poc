from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import json
from nlu.intent_schema import IntentResult, Intent

MODEL_NAME = "Qwen/Qwen2.5-1.5B-Instruct"
CACHE_DIR = "models"

_MODEL = None
_TOKENIZER = None


# =========================
# Qwen ëª¨ë¸ ë¡œë”© (1íšŒ)
# =========================
def load_qwen():
    global _MODEL, _TOKENIZER

    if _MODEL is None:
        print("â³ Qwen LLM ë¡œë”© ì¤‘...")

        _TOKENIZER = AutoTokenizer.from_pretrained(
            MODEL_NAME,
            trust_remote_code=True,
            cache_dir=CACHE_DIR,
        )

        _MODEL = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            device_map="cpu",
            torch_dtype=torch.float32,
            cache_dir=CACHE_DIR,
        )

        _MODEL.eval()
        print("âœ… Qwen LLM ë¡œë”© ì™„ë£Œ")

    return _MODEL, _TOKENIZER


# =========================
# Intent íŒë³„ (í™•ì¥ ë²„ì „)
# =========================
def detect_intent_llm(text: str, debug: bool = True) -> IntentResult:
    model, tokenizer = load_qwen()

    if debug:
        print(f"ğŸ“¥ [LLM INPUT] {text}")

    messages = [
        {
            "role": "system",
            "content": (
                "ë„ˆëŠ” 'ì£¼ì°¨ì¥ ì¶œì… ì°¨ë‹¨ê¸° ì œì–´' ì „ìš© AIë‹¤.\n\n"
                "ì‚¬ìš©ìì˜ ë°œí™”ë¥¼ ì•„ë˜ ì˜ë„ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ë¼:\n\n"
                "- OPEN_GATE: ì§€ê¸ˆ ë‹¹ì¥ ì°¨ë‹¨ê¸°ë¥¼ ì—´ì–´ë‹¬ë¼ëŠ” ëª…ì‹œì  ìš”ì²­\n"
                "- CLOSE_GATE: ì°¨ë‹¨ê¸°ë¥¼ ë‹«ê±°ë‚˜ ë§‰ì•„ë‹¬ë¼ëŠ” ëª…ì‹œì  ìš”ì²­\n"
                "- HELP_REQUEST: ë¬¸ì´ ì•ˆ ì—´ë¦¼, ê²°ì œ ì‹¤íŒ¨, ë“±ë¡ ì˜¤ë¥˜ ë“± ë¬¸ì œ ìƒí™© ì„¤ëª…\n"
                "- INFO_REQUEST: ë°©ë¬¸ ë“±ë¡ ë°©ë²•, ì ˆì°¨, ì‚¬ìš©ë²•ì„ ë¬»ëŠ” ì§ˆë¬¸\n"
                "- NONE: ì°¨ë‹¨ê¸° ì œì–´ì™€ ë¬´ê´€í•œ ë°œí™”\n\n"
                "âš ï¸ ë§¤ìš° ì¤‘ìš”:\n"
                "- OPEN_GATEëŠ” 'ì—´ì–´ì¤˜', 'ì˜¬ë ¤ì¤˜', 'í†µê³¼í• ê²Œìš”' ë“± ì§ì ‘ ëª…ë ¹ì¼ ë•Œë§Œ ì„ íƒí•œë‹¤\n"
                "- 'ë¬¸ì´ ì•ˆ ì—´ë ¤ìš”', 'ë°©ë¬¸ë“±ë¡ í–ˆëŠ”ë° ì•ˆë¼ìš”'ëŠ” OPEN_GATEê°€ ì•„ë‹ˆë¼ HELP_REQUESTë‹¤\n"
                "- ì§ˆë¬¸í˜• ë¬¸ì¥ì€ INFO_REQUESTë¡œ ë¶„ë¥˜í•œë‹¤\n"
                "- ì• ë§¤í•˜ë©´ ë°˜ë“œì‹œ NONE ë˜ëŠ” HELP_REQUESTë¥¼ ì„ íƒí•œë‹¤\n\n"
                "ì¶œë ¥ ê·œì¹™:\n"
                "- ë°˜ë“œì‹œ JSONë§Œ ì¶œë ¥í•œë‹¤\n"
                "- í˜•ì‹: {\"intent\":\"OPEN_GATE|CLOSE_GATE|HELP_REQUEST|INFO_REQUEST|NONE\",\"confidence\":0.0~1.0}"
            ),
        },
        {
            "role": "user",
            "content": text,
        },
    ]

    input_ids = tokenizer.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=True,
        return_tensors="pt",
    )

    with torch.no_grad():
        output_ids = model.generate(
            input_ids=input_ids,
            max_new_tokens=64,
            do_sample=False,
            eos_token_id=tokenizer.eos_token_id,
        )

    generated_ids = output_ids[0][input_ids.shape[-1]:]
    decoded = tokenizer.decode(generated_ids, skip_special_tokens=True)

    if debug:
        print("ğŸ§¾ [LLM RAW OUTPUT]")
        print(decoded)

    # =========================
    # JSON íŒŒì‹± + Enum ë³€í™˜
    # =========================
    try:
        start = decoded.find("{")
        end = decoded.rfind("}") + 1
        data = json.loads(decoded[start:end])

        intent_str = data.get("intent", "NONE")
        confidence = float(data.get("confidence", 0.0))

        try:
            intent = Intent(intent_str)
        except ValueError:
            intent = Intent.NONE

        confidence = max(0.0, min(confidence, 1.0))

        if debug:
            print(
                f"ğŸ“Š [LLM PARSED] intent={intent.name}, "
                f"confidence={confidence:.2f}"
            )

        return IntentResult(intent=intent, confidence=confidence)

    except Exception as e:
        if debug:
            print("âŒ [LLM PARSE ERROR]", e)
        return IntentResult(intent=Intent.NONE, confidence=0.0)
