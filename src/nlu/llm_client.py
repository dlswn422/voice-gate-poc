from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import json
from nlu.intent_schema import IntentResult, Intent

MODEL_NAME = "Qwen/Qwen2.5-1.5B-Instruct"
CACHE_DIR = "models"

_MODEL = None
_TOKENIZER = None


# =========================
# Qwen ëª¨ë¸ ë¡œë”© (1íšŒ)
# =========================
def load_qwen():
    global _MODEL, _TOKENIZER

    if _MODEL is None:
        print("â³ Qwen LLM ë¡œë”© ì¤‘...")

        _TOKENIZER = AutoTokenizer.from_pretrained(
            MODEL_NAME,
            trust_remote_code=True,
            cache_dir=CACHE_DIR,
        )

        _MODEL = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            device_map="cpu",
            torch_dtype=torch.float32,
            cache_dir=CACHE_DIR,
        )

        _MODEL.eval()
        print("âœ… Qwen LLM ë¡œë”© ì™„ë£Œ")

    return _MODEL, _TOKENIZER


# =========================
# Intent íŒë³„ (LLM ì¤‘ì‹¬, ìœ ì‚¬ ë°œìŒ í—ˆìš©)
# =========================
def detect_intent_llm(text: str, debug: bool = True) -> IntentResult:
    model, tokenizer = load_qwen()

    if debug:
        print(f"ğŸ“¥ [LLM INPUT] {text}")

    messages = [
        {
            "role": "system",
            "content": (
                "ë„ˆëŠ” 'ì£¼ì°¨ì¥ ì¶œì… ì°¨ë‹¨ê¸° ì œì–´' ì „ìš© AIë‹¤.\n\n"
                "ë„ˆì˜ ì„ë¬´ëŠ” ì‚¬ìš©ìì˜ ë°œí™”ê°€ ì•„ë˜ ì¤‘ ë¬´ì—‡ì¸ì§€ íŒë‹¨í•˜ëŠ” ê²ƒì´ë‹¤:\n"
                "- OPEN_GATE: ì‚¬ìš©ìê°€ ì§ì ‘ ì´ë™/ì¶œì…/í†µê³¼í•˜ê¸° ìœ„í•´ ì°¨ë‹¨ê¸°ë¥¼ ì—´ì–´ë‹¬ë¼ëŠ” ìš”ì²­\n"
                "- CLOSE_GATE: ì°¨ë‹¨ê¸°ë¥¼ ë‹«ê±°ë‚˜ ë§‰ì•„ë‹¬ë¼ëŠ” ìš”ì²­\n"
                "- NONE: ì°¨ë‹¨ê¸° ì œì–´ì™€ ë¬´ê´€í•˜ê±°ë‚˜, ì¶œì… ë§¥ë½ì´ ë¶ˆëª…í™•í•œ ë°œí™”\n\n"
                "âš ï¸ ë§¤ìš° ì¤‘ìš” (ì˜¤íƒ€ / ìœ ì‚¬ ë°œìŒ ì²˜ë¦¬ ê·œì¹™):\n"
                "- ìŒì„± ì¸ì‹ íŠ¹ì„±ìƒ ë‹¨ì–´ê°€ í‹€ë¦´ ìˆ˜ ìˆìŒì„ ë°˜ë“œì‹œ ê³ ë ¤í•œë‹¤\n"
                "- 'ì°¨ë‹¨ê¸°'ì™€ ë°œìŒÂ·ì² ìê°€ ìœ ì‚¬í•œ ë‹¨ì–´ëŠ” ë¬¸ë§¥ìƒ ì°¨ë‹¨ê¸°ë¥¼ ì˜ë¯¸í•  ìˆ˜ ìˆë‹¤\n"
                "  ì˜ˆ: ì°¨ë‹¹ê¸°, í•˜ë‹¨ê¸°, ì‚¬ë‹¨ê¸°, ì²˜ë‹¨ê¸°, ì± ë‹¨ê¸° ë“±\n"
                "- ë‹¨ì–´ ìì²´ë³´ë‹¤ 'ì¶œì…/í†µê³¼í•˜ë ¤ëŠ” ì˜ë„'ê°€ í•µì‹¬ì´ë‹¤\n\n"
                "íŒë‹¨ ê¸°ì¤€:\n"
                "- ì¶œì…/ì´ë™/í†µê³¼ ë§¥ë½ì´ ëª…í™•í•˜ë©´ OPEN_GATE\n"
                "- ë¬¼ë¦¬ì  ì¶œì… ì¥ì¹˜ë¥¼ ì˜¬ë¦¬ê±°ë‚˜ ì—´ì–´ë‹¬ë¼ëŠ” ì˜ë¯¸ë©´ OPEN_GATE\n"
                "- ë¬¼ë¦¬ì  ì¶œì… ì¥ì¹˜ë¥¼ ë‚´ë¦¬ê±°ë‚˜ ë§‰ì•„ë‹¬ë¼ëŠ” ì˜ë¯¸ë©´ CLOSE_GATE\n"
                "- ë‚´ë¶€ ê³µê°„(ë°©, ë°©ë¬¸, í™”ì¥ì‹¤, ì§‘, ì‚¬ë¬´ì‹¤ ë“±)ì€ ë¬´ì¡°ê±´ NONE\n"
                "- ì¡ë‹´, ì„¤ëª…, ì§ˆë¬¸, ê°ì • í‘œí˜„ì€ NONE\n"
                "- ì¡°ê¸ˆì´ë¼ë„ í™•ì‹ ì´ ë¶€ì¡±í•˜ë©´ ë°˜ë“œì‹œ NONE\n\n"
                "ì¶œë ¥ ê·œì¹™:\n"
                "- ë°˜ë“œì‹œ JSONë§Œ ì¶œë ¥í•œë‹¤\n"
                "- í˜•ì‹: {\"intent\":\"OPEN_GATE|CLOSE_GATE|NONE\",\"confidence\":0.0~1.0}\n"
            ),
        },
        {
            "role": "user",
            "content": text,
        },
    ]

    input_ids = tokenizer.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=True,
        return_tensors="pt",
    )

    with torch.no_grad():
        output_ids = model.generate(
            input_ids=input_ids,
            max_new_tokens=64,
            do_sample=False,  # ê²°ì •ë¡ ì 
            eos_token_id=tokenizer.eos_token_id,
        )

    # ìƒì„±ëœ ë¶€ë¶„ë§Œ ë””ì½”ë”©
    generated_ids = output_ids[0][input_ids.shape[-1]:]
    decoded = tokenizer.decode(generated_ids, skip_special_tokens=True)

    if debug:
        print("ğŸ§¾ [LLM RAW OUTPUT]")
        print(decoded)

    # =========================
    # JSON ì•ˆì „ íŒŒì‹± + Intent ë³€í™˜
    # =========================
    try:
        start = decoded.find("{")
        end = decoded.rfind("}") + 1
        data = json.loads(decoded[start:end])

        intent_str = data.get("intent", "NONE")
        confidence = float(data.get("confidence", 0.0))

        # ğŸ”‘ ë¬¸ìì—´ â†’ Enum ë³€í™˜ (í•µì‹¬)
        try:
            intent = Intent(intent_str)
        except ValueError:
            intent = Intent.NONE

        # confidence ë³´ì •
        confidence = max(0.0, min(confidence, 1.0))

        if debug:
            print(
                f"ğŸ“Š [LLM PARSED] intent={intent.name}, "
                f"confidence={confidence:.2f}"
            )

        return IntentResult(intent=intent, confidence=confidence)

    except Exception as e:
        if debug:
            print("âŒ [LLM PARSE ERROR]", e)
        return IntentResult(intent=Intent.NONE, confidence=0.0)
