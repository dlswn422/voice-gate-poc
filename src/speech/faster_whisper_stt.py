import sounddevice as sd
import numpy as np
from faster_whisper import WhisperModel
from typing import Optional, Callable


class FasterWhisperSTT:
    """
    ë°œí™” ë‹¨ìœ„ VAD ê¸°ë°˜ STT ì—”ì§„ (ì•ˆì • ìµœì¢…ë³¸)

    ë™ìž‘:
    - ë¬´ìŒ ëŒ€ê¸°
    - ìŒì„± ì‹œìž‘ ê°ì§€
    - ë°œí™” ëê¹Œì§€ ë²„í¼ë§
    - WhisperëŠ” ë°œí™”ë‹¹ 1íšŒ ì‹¤í–‰
    """

    def __init__(
        self,
        model_size: str = "large-v3",
        device_index: Optional[int] = None,
        sample_rate: int = 16000,
        chunk_seconds: float = 0.4,
        silence_threshold: float = 0.02,
        silence_chunks: int = 1,
    ):
        self.sample_rate = sample_rate
        self.chunk_seconds = chunk_seconds
        self.silence_threshold = silence_threshold
        self.silence_chunks = silence_chunks
        self.device_index = device_index

        print("[STT] Loading Faster-Whisper model...")
        self.model = WhisperModel(
            model_size,
            device="cpu",
            compute_type="int8",
            download_root="models",
        )
        print("[STT] Faster-Whisper model loaded")

        self.on_text: Optional[Callable[[str], None]] = None

    def start_listening(self):
        print("[STT] Listening started (Ctrl+C to stop)")

        # sounddevice ì´ˆê¸°í™”
        sd.stop()
        sd.default.device = self.device_index
        sd.default.samplerate = self.sample_rate
        sd.default.channels = 1

        buffer = []
        silent_count = 0
        is_speaking = False
        frames_per_chunk = int(self.chunk_seconds * self.sample_rate)

        try:
            while True:
                # ì˜¤ë””ì˜¤ ìˆ˜ì§‘
                audio = sd.rec(
                    frames_per_chunk,
                    dtype="float32",
                )
                sd.wait()

                audio = audio.squeeze()
                volume = np.max(np.abs(audio))

                # ðŸ” ë””ë²„ê·¸ìš© (í•„ìš” ì—†ìœ¼ë©´ ì§€ì›Œë„ ë¨)
                # print(f"[DEBUG] volume={volume:.4f}")

                # ------------------------------
                # ìŒì„± ì‹œìž‘ ê°ì§€
                # ------------------------------
                if volume >= self.silence_threshold:
                    if not is_speaking:
                        print("[STT] Speech detected")
                        is_speaking = True

                    buffer.append(audio)
                    silent_count = 0

                else:
                    if is_speaking:
                        silent_count += 1

                # ------------------------------
                # ë°œí™” ì¢…ë£Œ ê°ì§€
                # ------------------------------
                if is_speaking and silent_count >= self.silence_chunks:
                    print("[STT] Speech ended, running transcription")
                    self._process_buffer(buffer)

                    buffer.clear()
                    silent_count = 0
                    is_speaking = False

        except KeyboardInterrupt:
            sd.stop()
            print("[STT] Listening stopped")

        except Exception as e:
            sd.stop()
            print("[STT ERROR]", repr(e))

    def _process_buffer(self, buffer):
        if not buffer:
            return

        audio = np.concatenate(buffer)

        segments, _ = self.model.transcribe(
            audio,
            language="ko",
            beam_size=8,
            vad_filter=True,
        )

        text = "".join(seg.text for seg in segments).strip()

        if not text:
            print("[STT] No transcription result")
            return

        print(f"[STT] Transcribed text: {text}")

        if self.on_text:
            self.on_text(text)