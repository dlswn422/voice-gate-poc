import sounddevice as sd
import numpy as np
from faster_whisper import WhisperModel
from typing import Optional, Callable


class FasterWhisperSTT:
    """
    VAD ê¸°ë°˜ ë°œí™” ì¢…ë£Œ STT (ìµœì¢… ì•ˆì •ë³¸)
    - ë¬´ìŒ êµ¬ê°„ì—ì„œëŠ” ë¡œê·¸ ì¶œë ¥ ì—†ìŒ
    - ìŒì„± ì‹œì‘ / ë°œí™” ì¢…ë£Œ / STT ê²°ê³¼ë§Œ ì¶œë ¥
    - ë°œí™” ì¢…ë£Œ ì‹œì—ë§Œ STT ìˆ˜í–‰
    """

    def __init__(
        self,
        model_size: str = "large-v3",
        device_index: Optional[int] = None,
        sample_rate: int = 16000,
        chunk_seconds: float = 0.5,
        silence_threshold: float = 0.015,
        silence_chunks: int = 2,
    ):
        self.sample_rate = sample_rate
        self.chunk_seconds = chunk_seconds
        self.silence_threshold = silence_threshold
        self.silence_chunks = silence_chunks
        self.device_index = device_index

        print("â³ Faster-Whisper ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.model = WhisperModel(
            model_size,
            device="cpu",
            compute_type="int8",
            download_root="models",
        )
        print("âœ… Faster-Whisper ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

        self.on_text: Optional[Callable[[str], None]] = None

    def start_listening(self):
        print("ğŸ™ STT ì‹œì‘ (VAD ê¸°ë°˜, Ctrl+C ì¢…ë£Œ)")

        buffer = []
        silent_count = 0
        is_speaking = False

        try:
            with sd.InputStream(
                samplerate=self.sample_rate,
                device=self.device_index,
                channels=1,
                dtype="float32",
            ) as stream:

                while True:
                    data, _ = stream.read(
                        int(self.chunk_seconds * self.sample_rate)
                    )
                    audio = data.squeeze()
                    volume = np.max(np.abs(audio))

                    # ìŒì„± ê°ì§€
                    if volume >= self.silence_threshold:
                        if not is_speaking:
                            print("ğŸ—£ ìŒì„± ê°ì§€ ì‹œì‘")
                            is_speaking = True

                        buffer.append(audio)
                        silent_count = 0
                    else:
                        if is_speaking:
                            silent_count += 1

                    # ë°œí™” ì¢…ë£Œ íŒë‹¨
                    if is_speaking and silent_count >= self.silence_chunks:
                        print("ğŸ§¾ ë°œí™” ì¢…ë£Œ ê°ì§€ â†’ STT ìˆ˜í–‰")
                        self._process_buffer(buffer)
                        buffer.clear()
                        silent_count = 0
                        is_speaking = False

        except KeyboardInterrupt:
            print("\nğŸ›‘ STT ì¢…ë£Œ")

    def _process_buffer(self, buffer):
        if not buffer:
            return

        audio = np.concatenate(buffer)

        segments, _ = self.model.transcribe(
            audio,
            language="ko",
            beam_size=8,
            vad_filter=True,
        )

        text = "".join(seg.text for seg in segments).strip()

        if not text:
            print("âš ï¸ STT ê²°ê³¼ ì—†ìŒ")
            return

        print(f"ğŸ—£ [STT] {text}")

        if self.on_text:
            self.on_text(text)