import sounddevice as sd
import numpy as np
import time
from faster_whisper import WhisperModel
from typing import Optional, Callable


class FasterWhisperSTT:
    """
    VAD ê¸°ë°˜ ë°œí™” ì¢…ë£Œ STT (ìµœì¢…)
    - ë¬´ìŒ ê¸°ë°˜ ë°œí™” í™•ì •
    - ë°œí™” ëë‚¬ì„ ë•Œë§Œ LLM í˜¸ì¶œ
    """

    def __init__(
        self,
        model_size: str = "large-v3",
        device_index: Optional[int] = None,
        sample_rate: int = 16000,
        chunk_seconds: float = 0.5,
        silence_threshold: float = 0.015,
        silence_chunks: int = 2,
    ):
        self.sample_rate = sample_rate
        self.chunk_seconds = chunk_seconds
        self.silence_threshold = silence_threshold
        self.silence_chunks = silence_chunks
        self.device_index = device_index

        print("â³ Faster-Whisper ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.model = WhisperModel(
            model_size,
            device="cpu",
            compute_type="int8",
            download_root="models",
        )
        print("âœ… Faster-Whisper ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

        self.on_text: Optional[Callable[[str], None]] = None

    def start_listening(self):
        print("ğŸ™ STT ì‹œì‘ (VAD ê¸°ë°˜, Ctrl+C ì¢…ë£Œ)")

        buffer = []
        silent_count = 0

        try:
            with sd.InputStream(
                samplerate=self.sample_rate,
                device=self.device_index,
                channels=1,
                dtype="float32",
            ) as stream:

                while True:
                    data, _ = stream.read(int(self.chunk_seconds * self.sample_rate))
                    audio = data.squeeze()

                    volume = np.max(np.abs(audio))
                    print(f"ğŸ”Š volume={volume:.4f}")

                    if volume < self.silence_threshold:
                        silent_count += 1
                        print(f"ğŸ¤« ë¬´ìŒ ê°ì§€ ({silent_count}/{self.silence_chunks})")
                    else:
                        silent_count = 0
                        buffer.append(audio)
                        print("ğŸ—£ ìŒì„± ìˆ˜ì§‘ ì¤‘...")

                    # ë°œí™” ì¢…ë£Œ íŒë‹¨
                    if silent_count >= self.silence_chunks and buffer:
                        print("ğŸ§¾ ë°œí™” ì¢…ë£Œ ê°ì§€ â†’ STT ìˆ˜í–‰")
                        self._process_buffer(buffer)
                        buffer.clear()
                        silent_count = 0

        except KeyboardInterrupt:
            print("\nğŸ›‘ STT ì¢…ë£Œ")

    def _process_buffer(self, buffer):
        audio = np.concatenate(buffer)

        segments, _ = self.model.transcribe(
            audio,
            language="ko",
            beam_size=8,
            vad_filter=True,
        )

        text = "".join(seg.text for seg in segments).strip()

        if not text:
            print("âš ï¸ STT ê²°ê³¼ ì—†ìŒ")
            return

        print(f"ğŸ—£ [STT] {text}")

        if self.on_text:
            self.on_text(text)